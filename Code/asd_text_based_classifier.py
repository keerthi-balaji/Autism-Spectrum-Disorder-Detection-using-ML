# -*- coding: utf-8 -*-
"""ASD Text-Based Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O-H14Z41QNm0qS7YngTuaKy0LnBfH7U5
"""

# Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Loading the 3 datasets

data1 = pd.read_csv('data_csv.csv')
data2 = pd.read_csv('Toddler Autism dataset July 2018.csv')
data3 = pd.read_csv('autism_screening.csv')

#display dataset 1

display(data1)

#display dataset 2

display(data2)

#display dataset 3

display(data3)

#Remove uncommon columns/features

df1 = pd.concat([data1.iloc[:,1:11],data1.iloc[:,[12,22,23,24,25,26,27]]],axis=1)
print(df1.shape)
df1.head()

#Remove uncommon columns/features

df2 = pd.concat([data2.iloc[:,1:12],data2.iloc[:,13:]],axis=1)
df2['Age_Mons'] = (df2['Age_Mons']/12).astype(int)
print(df2.shape)
df2.head()

#Remove uncommon columns/features

df3=pd.concat([data3.iloc[:,0:15],data3.iloc[:,-2:]],axis=1)
print(df3.shape)
df3.head()

#display column names for all 3 datasets

order_test= pd.DataFrame({
    'df1': df1.columns,
    'df2': df2.columns ,
    'df3': df3.columns
})
order_test

# Rename columns to have the same names in all DataFrames
df2.columns = df3.columns = df1.columns

# Concatenate the DataFrames
data_fin = pd.concat([df3, df2, df1], axis=0)
data_fin.head()

#display the the number of rows and columns

data_fin.shape

# Get object type columns
object_cols = data_fin.select_dtypes('O').columns

# Create new DataFrame
object_df = pd.DataFrame({
    'Objects': object_cols,
    'Unique values': [data_fin[col].unique() for col in object_cols],
    'number of unique values':[data_fin[col].nunique()for col in object_cols]
})

object_df

#display the column names

data_fin.columns

#display the unique values for each feature

for col in ['Sex', 'Ethnicity',
       'Jaundice', 'Family_mem_with_ASD', 'Who_completed_the_test',
       'ASD_traits']:
    print("-------------------------------")
    print(f'Column name: {col}\n')
    print(f'Unique values:\n{data_fin[col].unique()}')

# Replace the different representations with uniform representation

replacements = {
    'f': 'F',
    'm': 'M',
}
data_fin['Sex'] = data_fin['Sex'].replace(replacements)

# Replace the different representations with uniform representation

replacements = {
    'yes': 'Yes',
    'no': 'No',
}
data_fin['Jaundice'] = data_fin['Jaundice'].replace(replacements)

# Replace the different representations with uniform representation

replacements = {
    'yes': 'Yes',
    'no': 'No',
}
data_fin['Family_mem_with_ASD'] = data_fin['Family_mem_with_ASD'].replace(replacements)

# Replace the different representations with uniform representation

replacements = {
    'YES': 'Yes',
    'NO': 'No',
}
data_fin['ASD_traits'] = data_fin['ASD_traits'].replace(replacements)

# Replace the different representations with uniform representation

replacements = {
    'middle eastern': 'Middle Eastern',
    'Middle Eastern ': 'Middle Eastern',
    'mixed': 'Mixed',
    'asian': 'Asian',
    'black': 'Black',
    'south asian': 'South Asian',
    'PaciFica':'Pacifica',
    'Pasifika':'Pacifica',
    'White-European':'White European'

}
data_fin['Ethnicity'] = data_fin['Ethnicity'].replace(replacements)

# Replace the different representations with uniform representation

replacements = {
    'Health care professional':'Health Care Professional',
    'family member':'Family Member',
    'Family member':'Family Member'
}
data_fin['Who_completed_the_test'] = data_fin['Who_completed_the_test'].replace(replacements)

#display the unique values for each feature

for col in ['Sex', 'Ethnicity',
       'Jaundice', 'Family_mem_with_ASD', 'Who_completed_the_test',
       'ASD_traits']:
    print("-------------------------------")
    print(f'Column name: {col}\n')
    print(f'Unique values:\n{data_fin[col].unique()}')

#replace '?' with 'NaN'

data_fin['Ethnicity'].replace('?', np.nan, inplace=True)
data_fin['Who_completed_the_test'].replace('?', np.nan, inplace=True)

# Display number of missing values for each column

pd.DataFrame(data_fin.isnull().sum(),
             columns=["Missing Values"]).style.bar(color = "#84A9AC")

# Using imputation to replace missing values with the most frequent values

idf=data_fin.copy()
from sklearn.impute import SimpleImputer
imp = SimpleImputer(strategy='most_frequent')
imputed_data = pd.DataFrame(imp.fit_transform(idf))
imputed_data.columns = idf.columns
imputed_data.index = idf.index

# Display number of missing values for each column

pd.DataFrame(imputed_data.isnull().sum(),
             columns=["Missing Values"]).style.bar(color = "#84A9AC")

# display the dataset

display(data_fin)

import seaborn as sns

# code to plot histograms for all numerical features
data_fin.hist(bins=20, figsize=(20, 15), alpha=1.0)
plt.suptitle("Histograms for Numerical Features", y=1.02)
plt.show()

# code to create a heatmap for correlation for all numerical features

corr_matrix = data_fin.corr(numeric_only=True)
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title("Correlation Heatmap")
plt.show()

# Plotting the bar chart for Ethnicity feature

ethnicity_counts = data_fin['Ethnicity'].value_counts()

ethnicity_counts.plot(kind='bar')

plt.xlabel('Ethnicity')
plt.ylabel('Count')
plt.title('Bar Chart for Ethnicity Counts')

plt.show()

# Plotting the bar chart for Jaundice feature

decision_counts = data_fin['Jaundice'].value_counts()

decision_counts.plot(kind='bar', color=['lightblue', 'pink'])

plt.xlabel('Jaundice')
plt.ylabel('Count')
plt.title('Bar Chart for Yes/No Counts')

# Plotting the bar chart for Sex feature

gender_counts = data_fin['Sex'].value_counts()

gender_counts.plot(kind='bar', color=['lightblue', 'pink'])

plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Bar Chart for Yes/No Counts')

# Plotting the bar chart for Family_mem_with_ASD feature

decision_counts = data_fin['Family_mem_with_ASD'].value_counts()

decision_counts.plot(kind='bar', color=['lightgrey', 'lightgreen'])

plt.xlabel('Family member with ASD')
plt.ylabel('Count')
plt.title('Bar Chart for Yes/No Counts')

# Plotting the bar chart for Who_completed_the_test feature

Who_completed_the_test_counts = data_fin['Who_completed_the_test'].value_counts()

Who_completed_the_test_counts.plot(kind='bar')

plt.xlabel('Who Completed the Test')
plt.ylabel('Count')
plt.title('Bar Chart for Who Completed the Test')

plt.show()

#Using Label Encoding to convert categorical values into numerical values

from sklearn import preprocessing

df = data_fin

label_encoder = preprocessing.LabelEncoder()
df['Sex'] = label_encoder.fit_transform(df['Sex'])
df['Ethnicity'] = label_encoder.fit_transform(df['Ethnicity'])
df['Jaundice'] = label_encoder.fit_transform(df['Jaundice'])
df['Family_mem_with_ASD'] = label_encoder.fit_transform(df['Family_mem_with_ASD'])
df['Who_completed_the_test'] = label_encoder.fit_transform(df['Who_completed_the_test'])
df['ASD_traits'] = label_encoder.fit_transform(df['ASD_traits'])

# display the dataset

display(data_fin)

#Selecting features using information gain based on entropy

from sklearn.model_selection import train_test_split


def calculate_entropy(y):

    unique_classes, class_counts = np.unique(y, return_counts=True)

    probabilities = class_counts / len(y)

    entropy = -np.sum(probabilities * np.log2(probabilities))

    return entropy

def calculate_information_gain(X, y, feature):

    # Calculate the entropy of the entire dataset

    total_entropy = calculate_entropy(y)

    # Calculate the weighted sum of entropies after the split

    unique_values = X[feature].unique()

    weighted_entropy_after_split = 0

    for value in unique_values:

        subset_y = y[X[feature] == value]

        weighted_entropy_after_split += (len(subset_y) / len(y)) * calculate_entropy(subset_y)

    # Calculate information gain

    information_gain = total_entropy - weighted_entropy_after_split

    return information_gain


df = data_fin

target_column = 'ASD_traits'

k_value = 10

df.fillna(0, inplace=True)


# Separate features and target variable

X = df.drop(target_column, axis=1)

y = df[target_column]

# Split the data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Calculate information gain for each feature

info_gains = X_train.apply(lambda x: calculate_information_gain(X_train, y_train, x.name))

# Select the top features based on information gain

selected_features_entropy = info_gains.sort_values(ascending=False).head(k_value).index

#display the selected features
print(selected_features_entropy)

#Selecting features using information gain based on Gini Index

def calculate_gini_index(y):

    unique_classes, class_counts = np.unique(y, return_counts=True)

    probabilities = class_counts / len(y)

    gini_index = 1 - np.sum(probabilities**2)

    return gini_index

def calculate_information_gain_gini(X, y, feature):

    # Calculate the Gini Index of the entire dataset

    total_gini_index = calculate_gini_index(y)

    # Calculate the weighted sum of Gini Index after the split

    unique_values = X[feature].unique()

    weighted_gini_index_after_split = 0

    for value in unique_values:

        subset_y = y[X[feature] == value]

        weighted_gini_index_after_split += (len(subset_y) / len(y)) * calculate_gini_index(subset_y)

    # Calculate information gain using Gini Index

    information_gain_gini = total_gini_index - weighted_gini_index_after_split

    return information_gain_gini

df = data_fin

target_column = 'ASD_traits'

k_value = 10

df.fillna(0, inplace=True)

# Separate features and target variable

X = df.drop(target_column, axis=1)

y = df[target_column]


# Split the data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Calculate information gain using Gini Index for each feature

info_gains_gini = X_train.apply(lambda x: calculate_information_gain_gini(X_train, y_train, x.name))

# Select the top features based on information gain using Gini Index

selected_features_gini = info_gains_gini.sort_values(ascending=False).head(k_value).index

# display the selected features
print(selected_features_gini)

# Employing SVM model on features selected using info gain based on entropy

from sklearn.svm import SVC

from sklearn.metrics import accuracy_score

# Train SVM model using selected features

svm_model = SVC()

svm_model.fit(X_train, y_train)

# Make predictions on the test set

y_pred = svm_model.predict(X_test)

# Evaluate the model

svm_accuracy_entropy = accuracy_score(y_test, y_pred)*100

print(f'Accuracy: {svm_accuracy_entropy}')

# Employing SVM model on features selected using info gain based on gini index

# Train SVM model using selected features

svm_model = SVC()

svm_model.fit(X_train[selected_features_gini], y_train)

# Make predictions on the test set

y_pred = svm_model.predict(X_test[selected_features_gini])

# Evaluate the model

svm_accuracy_gini = accuracy_score(y_test, y_pred)*100

print(f'Accuracy: {svm_accuracy_gini}')

# Plotting the bar chart to compare the 2 accuracies for SVM model

x = ['svm_accuracy_entropy', 'svm_accuracy_gini']
accuracy = [svm_accuracy_entropy, svm_accuracy_gini]


plt.figure(figsize=(3, 4))
plt.bar(x, accuracy, color='pink')
plt.xlabel('SVM ')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy of Support Vector Machine Model')
plt.ylim(0, 100)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.show()

# Employing ANN model on features selected using info gain based on entropy

# Set random seed for reproducibility
seed = 42
np.random.seed(seed)

target_column = 'ASD_traits'

# Separate features and target variable
X = df[selected_features_entropy]
# X = df.drop(target_column, axis=1)
y = df[target_column]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define an ANN model using Keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Initialize the ANN model
ann_model = Sequential()

# Add input layer and the first hidden layer
ann_model.add(Dense(units=32, activation='relu', input_dim=X_train.shape[1]))

# Add the output layer
ann_model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the ANN model using the selected features
ann_model.fit(X_train[selected_features_entropy], y_train, epochs=50, batch_size=32, validation_split=0.2)

# Make predictions on the test set
y_pred_proba = ann_model.predict(X_test[selected_features_entropy])
y_pred = (y_pred_proba > 0.5).astype(int)

# Evaluate the model
ann_accuracy_entropy = accuracy_score(y_test, y_pred)*100
print(f'Accuracy: {ann_accuracy_entropy}')

# Employing ANN model on features selected using info gain based on gini index

# Set random seed for reproducibility
seed = 42
np.random.seed(seed)

target_column = 'ASD_traits'

# Separate features and target variable
X = df[selected_features_gini]
# X = df.drop(target_column, axis=1)
y = df[target_column]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the ANN model
ann_model = Sequential()

# Add input layer and the first hidden layer
ann_model.add(Dense(units=32, activation='relu', input_dim=X_train.shape[1]))

# Add the output layer
ann_model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the ANN model using the selected features
ann_model.fit(X_train[selected_features_gini], y_train, epochs=50, batch_size=32, validation_split=0.2)

# Make predictions on the test set
y_pred_proba = ann_model.predict(X_test[selected_features_gini])
y_pred = (y_pred_proba > 0.5).astype(int)

# Evaluate the model
ann_accuracy_gini = accuracy_score(y_test, y_pred)*100
print(f'Accuracy: {ann_accuracy_gini}')

# Plotting the bar chart to compare the 2 accuracies for ANN model

x = ['ann_accuracy_entropy', 'ann_accuracy_gini']
accuracy = [ann_accuracy_entropy, ann_accuracy_gini]


plt.figure(figsize=(3, 4))
plt.bar(x, accuracy, color='lightblue')
plt.xlabel('ANN')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy of Artificial Neural Network Model')
plt.ylim(0, 100)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.show()

# Employing MLP model on features selected using info gain based on entropy

from sklearn.neural_network import MLPClassifier
from sklearn.feature_selection import SelectKBest, f_classif

# Set random seed for reproducibility
seed = 42
np.random.seed(seed)

# Separate features and target variable
X = df[selected_features_entropy]
y = df[target_column]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize MLPClassifier
mlp_model = MLPClassifier(hidden_layer_sizes=(32,), max_iter=500, random_state=42)

# Train the MLP model
mlp_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = mlp_model.predict(X_test)

# Evaluate the model
mlp_accuracy_entropy = accuracy_score(y_test, y_pred)*100
print(f'Accuracy: {mlp_accuracy_entropy}')

# Employing MLP model on features selected using info gain based on gini index

# Set random seed for reproducibility
seed = 42
np.random.seed(seed)

# Separate features and target variable
X = df[selected_features_gini]
y = df[target_column]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize MLPClassifier
mlp_model = MLPClassifier(hidden_layer_sizes=(32,), max_iter=500, random_state=42)

# Train the MLP model
mlp_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = mlp_model.predict(X_test)

# Evaluate the model
mlp_accuracy_gini = accuracy_score(y_test, y_pred)*100
print(f'Accuracy: {mlp_accuracy_gini}')

# Plotting the bar chart to compare the 2 accuracies for MLP model

x = [ 'mlp_accuracy_entropy', 'mlp_accuracy_gini']
accuracy = [ mlp_accuracy_entropy, mlp_accuracy_gini]


plt.figure(figsize=(3, 4))
plt.bar(x, accuracy, color='lightcoral')
plt.xlabel('MLP')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy of Multilayer Perceptron Model')
plt.ylim(0, 100)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.show()

# Plotting the bar chart to compare the 6 accuracies

x = ['svm_accuracy_entropy', 'svm_accuracy_gini', 'ann_accuracy_entropy', 'ann_accuracy_gini', 'mlp_accuracy_entropy', 'mlp_accuracy_gini']
accuracy = [svm_accuracy_entropy, svm_accuracy_gini, ann_accuracy_entropy, ann_accuracy_gini, mlp_accuracy_entropy, mlp_accuracy_gini]


plt.figure(figsize=(10, 6))
plt.bar(x, accuracy, color='pink')
plt.xlabel('Models and Criteria')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy of Different Models and Criteria')
plt.ylim(0, 100)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.show()

# code to save final dataset after carrying out the ML Experiment

import os

# Defining the target directory
target_directory = '/content/drive/MyDrive/'

os.makedirs(target_directory, exist_ok=True)

# Save the DataFrame to the CSV file
data_fin.to_csv(os.path.join(target_directory, 'data_fin.csv'), index=False)